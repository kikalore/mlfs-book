{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1f2881-7273-45ff-9259-537b23ad8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Root dir: /home/federica_lorenzini/ml-lab1/mlfs-book\n",
      "Added the following directory to the PYTHONPATH: /home/federica_lorenzini/ml-lab1/mlfs-book\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` \n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "\n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "from mlfs import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6a80c",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'>Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f447120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "from mlfs.airquality import util\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b8d1a-62a5-4a1d-b805-6e83cafcd29f",
   "metadata": {},
   "source": [
    "### Login into Hopsworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1a49d6-9cd2-4246-b0ca-1058672e4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:52:37,289 INFO: Initializing external client\n",
      "2025-11-16 14:52:37,290 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:52:39,199 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279175\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c023d8",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Get the AQICN_URL and API key. Enter country, city, street names for your Sensor.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9145f0b7-d961-41f7-aebe-741dbf00784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully found at the path: /home/federica_lorenzini/ml-lab1/mlfs-book/data/air-quality-data.csv\n",
      "2025-11-16 14:52:40,400 INFO: Received an HTTP error (403): <html>\n",
      "<head>\n",
      "<title>Access blocked</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>Access blocked</h1>\n",
      "\n",
      "<p>You have been blocked because you have violated the\n",
      "<a href=\"https://operations.osmfoundation.org/policies/nominatim/\">usage policy</a>\n",
      "of OSM's Nominatim geocoding service. Please be aware that OSM's resources are\n",
      "limited and shared between many users. The usage policy is there to ensure that\n",
      "the service remains usable for everybody.</p>\n",
      "\n",
      "<p>Please review the terms and make sure that your\n",
      "software adheres to the terms. You should in particular verify that you have set a\n",
      "<b>custom HTTP referrer or HTTP user agent</b> that identifies your application, and\n",
      "that you are not overusing the service with massive bulk requests.</p>\n",
      "\n",
      "<p>If you feel that this block is unjustified or remains after you have adopted\n",
      "your usage, you may contact the Nominatim system administrator at\n",
      "nominatim@openstreetmap.org to have this block lifted.</p>\n",
      "</body>\n",
      "</head>\n",
      "\n"
     ]
    },
    {
     "ename": "GeocoderInsufficientPrivileges",
     "evalue": "Non-successful status code 403",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAdapterHTTPError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-lab1/mlfs-book/.venv/lib/python3.12/site-packages/geopy/geocoders/base.py:368\u001b[39m, in \u001b[36mGeocoder._call_geocoder\u001b[39m\u001b[34m(self, url, callback, timeout, is_json, headers)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_json:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-lab1/mlfs-book/.venv/lib/python3.12/site-packages/geopy/adapters.py:472\u001b[39m, in \u001b[36mRequestsAdapter.get_json\u001b[39m\u001b[34m(self, url, timeout, headers)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, *, timeout, headers):\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-lab1/mlfs-book/.venv/lib/python3.12/site-packages/geopy/adapters.py:500\u001b[39m, in \u001b[36mRequestsAdapter._request\u001b[39m\u001b[34m(self, url, timeout, headers)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resp.status_code >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AdapterHTTPError(\n\u001b[32m    501\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNon-successful status code \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % resp.status_code,\n\u001b[32m    502\u001b[39m             status_code=resp.status_code,\n\u001b[32m    503\u001b[39m             headers=resp.headers,\n\u001b[32m    504\u001b[39m             text=resp.text,\n\u001b[32m    505\u001b[39m         )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[31mAdapterHTTPError\u001b[39m: Non-successful status code 403",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGeocoderInsufficientPrivileges\u001b[39m            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m city = \u001b[33m\"\u001b[39m\u001b[33mSollentuna\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m street = \u001b[33m\"\u001b[39m\u001b[33mekmans-vag-11\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m latitude, longitude = \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_city_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound AQICN_API_KEY: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAQICN_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m secrets = hopsworks.get_secrets_api()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-lab1/mlfs-book/mlfs/airquality/util.py:125\u001b[39m, in \u001b[36mget_city_coordinates\u001b[39m\u001b[34m(city_name)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Initialize Nominatim API (for getting lat and long of the city)\u001b[39;00m\n\u001b[32m    124\u001b[39m geolocator = Nominatim(user_agent=\u001b[33m\"\u001b[39m\u001b[33mMyApp\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m city = \u001b[43mgeolocator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgeocode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m latitude = \u001b[38;5;28mround\u001b[39m(city.latitude, \u001b[32m2\u001b[39m)\n\u001b[32m    128\u001b[39m longitude = \u001b[38;5;28mround\u001b[39m(city.longitude, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-lab1/mlfs-book/.venv/lib/python3.12/site-packages/geopy/geocoders/nominatim.py:297\u001b[39m, in \u001b[36mNominatim.geocode\u001b[39m\u001b[34m(self, query, exactly_one, timeout, limit, addressdetails, language, geometry, extratags, country_codes, viewbox, bounded, featuretype, namedetails)\u001b[39m\n\u001b[32m    295\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.geocode: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, url)\n\u001b[32m    296\u001b[39m callback = partial(\u001b[38;5;28mself\u001b[39m._parse_json, exactly_one=exactly_one)\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_geocoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-lab1/mlfs-book/.venv/lib/python3.12/site-packages/geopy/geocoders/base.py:388\u001b[39m, in \u001b[36mGeocoder._call_geocoder\u001b[39m\u001b[34m(self, url, callback, timeout, is_json, headers)\u001b[39m\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m callback(result)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_adapter_error_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m NONE_RESULT:\n\u001b[32m    390\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-lab1/mlfs-book/.venv/lib/python3.12/site-packages/geopy/geocoders/base.py:411\u001b[39m, in \u001b[36mGeocoder._adapter_error_handler\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc_cls(\n\u001b[32m    408\u001b[39m             \u001b[38;5;28mstr\u001b[39m(error), retry_after=get_retry_after(error.headers)\n\u001b[32m    409\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc_cls(\u001b[38;5;28mstr\u001b[39m(error)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    413\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._geocoder_exception_handler(error)\n",
      "\u001b[31mGeocoderInsufficientPrivileges\u001b[39m: Non-successful status code 403"
     ]
    }
   ],
   "source": [
    "today = datetime.date.today()\n",
    "\n",
    "csv_file=f\"{root_dir}/data/air-quality-data.csv\"\n",
    "util.check_file_path(csv_file)\n",
    "\n",
    "# taken from ~/.env.\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"You need to set AQICN_API_KEY either in this cell or in ~/.env\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value() \n",
    "aqicn_url = \"https://api.waqi.info/feed/@13983\"\n",
    "country = \"Sweden\"\n",
    "city = \"Sollentuna\"\n",
    "street = \"ekmans-vag-11\"\n",
    "latitude, longitude = util.get_city_coordinates(city)\n",
    "\n",
    "print(f\"Found AQICN_API_KEY: {AQICN_API_KEY}\")\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "# Replace any existing secret with the new value\n",
    "secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "if secret is not None:\n",
    "    secret.delete()\n",
    "    print(\"Replacing existing AQICN_API_KEY\")\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1c5eb-4a22-45d4-8934-f521e54a87c4",
   "metadata": {},
   "source": [
    "### Validate that the AQICN_API_KEY works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786127b0-c4e5-4a5f-a6fa-4cce903a9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aq_today_df = util.get_pm25(aqicn_url, country, city, street, today, AQICN_API_KEY)\n",
    "except hopsworks.RestAPIError:\n",
    "    print(\"It looks like the AQICN_API_KEY doesn't work for your sensor. Is the API key correct? Is the sensor URL correct?\")\n",
    "\n",
    "aq_today_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706e751",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Read your CSV file into a DataFrame </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a1212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file,  parse_dates=['date'], skipinitialspace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812eb37-04e3-4291-8d77-a69ef7a195bc",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Data cleaning</span>\n",
    "\n",
    "We want to have a DataFrame with 2 columns - `date` and `pm25` after this cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcfa73",
   "metadata": {},
   "source": [
    "### Check the data types for the columns in your DataFrame\n",
    "\n",
    " * `date` should be of type   datetime64[ns] \n",
    " * `pm25` should be of type float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20c859-ef3c-4b54-bbcb-83898afefa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq = df[['date', 'pm25']]\n",
    "df_aq['pm25'] = df_aq['pm25'].astype('float32')\n",
    "\n",
    "df_aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the pm25 column to be a float32 data type\n",
    "df_aq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19911f73",
   "metadata": {},
   "source": [
    "### Drop any rows with missing data </span>\n",
    "It will make the model training easier if there is no missing data in the rows, so we drop any rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq.dropna(inplace=True)\n",
    "df_aq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089c159",
   "metadata": {},
   "source": [
    "### Add country, city, street, url to the DataFrame\n",
    "\n",
    "We add the columns for the country, city, and street names that you changed for your Air Quality sensor.\n",
    "\n",
    "We also want to make sure the `pm25` column is a float32 data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the date and pm25 columns\n",
    "# If the column names in your DataFrame are different, rename your columns to `date` and `pm25`\n",
    "df_aq['country']=country\n",
    "df_aq['city']=city\n",
    "df_aq['street']=street\n",
    "df_aq['url']=aqicn_url\n",
    "df_aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6dc05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055befa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style='color:#ff5f27'> Loading Weather Data from [Open Meteo](https://open-meteo.com/en/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78686a28",
   "metadata": {},
   "source": [
    "### Download the Historical Weather Data\n",
    "\n",
    "https://open-meteo.com/en/docs/historical-weather-api#hourly=&daily=temperature_2m_mean,precipitation_sum,wind_speed_10m_max,wind_direction_10m_dominant\n",
    "\n",
    "We will download the historical weather data for your `city` by first extracting the earliest date from your DataFrame containing the historical air quality measurements.\n",
    "\n",
    "We will download all daily historical weather data measurements for your `city` from the earliest date in your air quality measurement DataFrame. It doesn't matter if there are missing days of air quality measurements. We can store all of the daily weather measurements, and when we build our training dataset, we will join up the air quality measurements for a given day to its weather features for that day. \n",
    "\n",
    "The weather features we will download are:\n",
    "\n",
    " * `temperature (average over the day)`\n",
    " * `precipitation (the total over the day)`\n",
    " * `wind speed (average over the day)`\n",
    " * `wind direction (the most dominant direction over the day)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d604b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_aq_date = pd.Series.min(df_aq['date'])\n",
    "earliest_aq_date = earliest_aq_date.strftime('%Y-%m-%d')\n",
    "earliest_aq_date\n",
    "\n",
    "weather_df = util.get_historical_weather(city, earliest_aq_date, str(today), latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6eefe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d5eeb",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Define Data Validation Rules </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db7bdc",
   "metadata": {},
   "source": [
    "### Expectations for Weather Data\n",
    "We will validate the air quality measurements (`pm25` values) before we write them to Hopsworks.\n",
    "\n",
    "We define a data validation rule (an expectation in Great Expectations) that ensures that `pm25` values are not negative or above the max value available by the sensor.\n",
    "\n",
    "We will attach this expectation to the air quality feature group, so that we validate the `pm25` data every time we write a DataFrame to the feature group. We want to prevent garbage-in, garbage-out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcdcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "aq_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":\"pm25\",\n",
    "            \"min_value\":-0.1,\n",
    "            \"max_value\":500.0,\n",
    "            \"strict_min\":True\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef9ed3",
   "metadata": {},
   "source": [
    "### Expectations for Weather Data\n",
    "Here, we define an expectation for 2 columns in our weather DataFrame - `precipitation_sum` and `wind_speed_10m_max`, where we expect both values to be greater than zero, but less than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "weather_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "def expect_greater_than_zero(col):\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\":col,\n",
    "                \"min_value\":-0.1,\n",
    "                \"max_value\":1000.0,\n",
    "                \"strict_min\":True\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "expect_greater_than_zero(\"precipitation_sum\")\n",
    "expect_greater_than_zero(\"wind_speed_10m_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3830b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291a502",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> Connect to Hopsworks and save the sensor country, city, street names as a secret</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd82f7",
   "metadata": {},
   "source": [
    "#### Save country, city, street names as a secret\n",
    "\n",
    "These will be downloaded from Hopsworks later in the (1) daily feature pipeline and (2) the daily batch inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_obj = {\n",
    "    \"country\": country,\n",
    "    \"city\": city,\n",
    "    \"street\": street,\n",
    "    \"aqicn_url\": aqicn_url,\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a JSON string\n",
    "str_dict = json.dumps(dict_obj)\n",
    "\n",
    "# Replace any existing secret with the new value\n",
    "secret = secrets.get_secret(\"SENSOR_LOCATION_JSON\")\n",
    "if secret is not None:\n",
    "    secret.delete()\n",
    "    print(\"Replacing existing SENSOR_LOCATION_JSON\")\n",
    "\n",
    "secrets.create_secret(\"SENSOR_LOCATION_JSON\", str_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e79b3f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> Create the Feature Groups and insert the DataFrames in them </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3755b6f",
   "metadata": {},
   "source": [
    "### Air Quality Data\n",
    "    \n",
    " 1. Provide a name, description, and version for the feature group.\n",
    " 2. Define the `primary_key`: we have to select which columns uniquely identify each row in the DataFrame - by providing them as the `primary_key`. Here, each air quality sensor measurement is uniquely identified by `country`, `street`, and  `date`.\n",
    " 3. Define the `event_time`: We also define which column stores the timestamp or date for the row - `date`.\n",
    " 4. Attach any `expectation_suite` containing data validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bb403",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "air_quality_fg = fs.get_or_create_feature_group(\n",
    "    name='air_quality',\n",
    "    description='Air Quality characteristics of each day',\n",
    "    version=1,\n",
    "    primary_key=['country','city', 'street'],\n",
    "    event_time=\"date\",\n",
    "    expectation_suite=aq_expectation_suite\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933cfa5",
   "metadata": {},
   "source": [
    "#### Insert the DataFrame into the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb42574",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg.insert(df_aq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a1606",
   "metadata": {},
   "source": [
    "#### Enter a description for each feature in the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577effca",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg.update_feature_description(\"date\", \"Date of measurement of air quality\")\n",
    "air_quality_fg.update_feature_description(\"country\", \"Country where the air quality was measured (sometimes a city in acqcn.org)\")\n",
    "air_quality_fg.update_feature_description(\"city\", \"City where the air quality was measured\")\n",
    "air_quality_fg.update_feature_description(\"street\", \"Street in the city where the air quality was measured\")\n",
    "air_quality_fg.update_feature_description(\"pm25\", \"Particles less than 2.5 micrometers in diameter (fine particles) pose health risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894b731",
   "metadata": {},
   "source": [
    "### Weather Data\n",
    "    \n",
    " 1. Provide a name, description, and version for the feature group.\n",
    " 2. Define the `primary_key`: we have to select which columns uniquely identify each row in the DataFrame - by providing them as the `primary_key`. Here, each weather measurement is uniquely identified by `city` and  `date`.\n",
    " 3. Define the `event_time`: We also define which column stores the timestamp or date for the row - `date`.\n",
    " 4. Attach any `expectation_suite` containing data validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create feature group \n",
    "weather_fg = fs.get_or_create_feature_group(\n",
    "    name='weather',\n",
    "    description='Weather characteristics of each day',\n",
    "    version=1,\n",
    "    primary_key=['city'],\n",
    "    event_time=\"date\",\n",
    "    expectation_suite=weather_expectation_suite\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721881b7",
   "metadata": {},
   "source": [
    "#### Insert the DataFrame into the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba846ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert data\n",
    "weather_fg.insert(weather_df, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87422d",
   "metadata": {},
   "source": [
    "#### Enter a description for each feature in the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_fg.update_feature_description(\"date\", \"Date of measurement of weather\")\n",
    "weather_fg.update_feature_description(\"city\", \"City where weather is measured/forecast for\")\n",
    "weather_fg.update_feature_description(\"temperature_2m_mean\", \"Temperature in Celsius\")\n",
    "weather_fg.update_feature_description(\"precipitation_sum\", \"Precipitation (rain/snow) in mm\")\n",
    "weather_fg.update_feature_description(\"wind_speed_10m_max\", \"Wind speed at 10m abouve ground\")\n",
    "weather_fg.update_feature_description(\"wind_direction_10m_dominant\", \"Dominant Wind direction over the dayd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
